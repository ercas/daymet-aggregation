#!/usr/bin/env python3
#
# Convert a file of extreme weather indicators generated by
# stage3_extreme_weather.R into a file of hot and cold waves and singleton
# extreme weather events, where waves are defined as two or more days of
# extreme weather concurrently and a singleton extreme weather event is a day
# of extreme weather not followed or preceded  by another day of extreme
# weather.
#
# The output file will have a unique ID for each wave or singleton extreme
# weather event, the length of the wave or "1" for singleton events, and the
# index of the day in the wave e.g. "1" for the 1st day, "2" for the 2nd day,
# etc.
#
# Contact: Edgar Castro <edgar_castro@g.harvard.edu>

import csv
import datetime
import gzip
import os
import typing

import tqdm


DEFAULT_INPUT_FILENAME = "extreme_temp_days.csv.gz"
DEFAULT_OUTPUT_FILENAME = "extreme_temp_waves.csv.gz"


def detect_id_column(path: str) -> str:
    with gzip.open(path, "rt") as input_fp:
        return next(csv.reader(input_fp))[0]


def extract_waves(input_path: str,
                  output_path: str,
                  id_field: str,
                  output_fieldnames: typing.List[str]
                  ) -> None:
    temp_path = output_path + ".temp"

    with gzip.open(input_path, "rt") as input_fp, gzip.open(temp_path, "wt") as output_fp:
        reader = csv.DictReader(input_fp)
        writer = csv.DictWriter(output_fp, fieldnames=output_fieldnames)

        previous_date = datetime.datetime(1, 1, 1)
        previous_extreme = None
        previous_id = None

        date_stack = []

        writer.writeheader()

        wave_id = 0

        for line in tqdm.tqdm(reader, desc="Parsed", unit=" rows"):
            this_date = datetime.datetime(
                int(line["year"]),
                int(line["month"]),
                int(line["day"])
            )
            this_id = line[id_field]
            this_extreme = line["extreme"]

            # if the ID changes or there is a gap of more than a day, dump the
            # stack
            if (
                (this_id != previous_id)
                or ((this_date - previous_date).days > 1)
            ):
                wave_length = len(date_stack)
                if wave_length > 0:
                    wave_id += 1
                    for (i, date) in enumerate(date_stack):
                        result = {
                            id_field: previous_id,
                            "year": date.year,
                            "month": date.month,
                            "day": date.day,
                            "extreme": previous_extreme,
                            "wave_id": wave_id,
                            "wave_index": i + 1,
                            "wave_length": wave_length
                        }
                        writer.writerow(result)
                date_stack = []

            # grow the stack
            date_stack.append(this_date)

            previous_date = this_date
            previous_id = this_id
            previous_extreme = this_extreme

        # dump last stack
        for (i, date) in enumerate(date_stack):
            result = {
                id_field: previous_id,
                "year": date.year,
                "month": date.month,
                "day": date.day,
                "extreme": previous_extreme,
                "wave_id": wave_id,
                "wave_index": i + 1,
                "wave_length": wave_length
            }
            #print(result)
            writer.writerow(result)

    os.rename(temp_path, output_path)


def extract_waves_wrapper(input_path: str, output_path: str) -> None:
    id_field = detect_id_column(input_path)

    output_fieldnames = [
        id_field, "year", "month", "day", "extreme", "wave_id", "wave_index",
        "wave_length"
    ]

    extract_waves(
        input_path, output_path, id_field, output_fieldnames
    )


if __name__ == "__main__":
    import argparse
    import glob

    parser = argparse.ArgumentParser()
    parser.add_argument("input", nargs="?", default=None)
    parser.add_argument("-o", "--output", default=None)
    args = parser.parse_args()


    if not args.output:
        args.output = DEFAULT_OUTPUT_FILENAME

    # input given: convert the given file
    if args.input:
        extract_waves_wrapper(args.input, args.output)

    # no input given: determine what files need to be converted by looking for
    # missing files in the expected daymet-aggregation output directory
    # hierarchy
    else:
        extra_directories = glob.glob("output/extra/*")
        for extra_directory in extra_directories:
            files = os.listdir(extra_directory)
            input_path = os.path.join(extra_directory, DEFAULT_INPUT_FILENAME)
            output_path = os.path.join(extra_directory, DEFAULT_OUTPUT_FILENAME)
            if os.path.isfile(output_path):
                print("Skipping {}".format(extra_directory))
            else:
                if os.path.isfile(input_path):
                    print("Generating {}".format(output_path))
                    extract_waves_wrapper(input_path, output_path)
                else:
                    print("ERROR: {} is missing".format(input_path))
                    continue

